{"cells":[{"metadata":{"_uuid":"f919d10e2ae012ae8d1c3fc1d81fe46452686609"},"cell_type":"markdown","source":"# House prices\n\n## Todo\n\n* Finish feature importance graph\n    * Convert to DMatrix to maintain feature names :/\n    * Remove one-hot encoded features if they add too much noise?\n* Remove any features with which less than 50% (?) of rows have values for\n* Feature scaling\n* Try`seaborn` graphs\n* Better analysis methods\n* (?) Use built-in pipeline CV rather than splitting manually\n    * Hope this cuts down code complexity (slightly)\n    * What graphs does this allow me to make?\n* Merging train & validation data for final model seems to make the final model worse, not better! Why?\n* Backup this kernel on Github\n    * Use the [Kaggle API](https://github.com/Kaggle/kaggle-api) tool to edit locally & run against remote images\n    * Script to pull all my kernels and back them up? Want to save the notebooks and the datasets ideally (maybe not the data if it's huge)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Imports\n\nimport warnings\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom category_encoders import OneHotEncoder\nfrom xgboost.sklearn import XGBRegressor\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nsns.set(style=\"white\", font_scale=0.9)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"289e6c5e68ba126fd0a5569dbd24a6d4960b0e0d"},"cell_type":"code","source":"# Load data\n\nrandom_state=3\ninitial_data = pd.read_csv(\"../input/train.csv\")\ntest_data = pd.read_csv(\"../input/test.csv\")\n\n# Align (unencoded) categoricals \ncategory_names = ['MSSubClass', 'MSZoning', 'Street',\n       'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig',\n       'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n       'HouseStyle', 'OverallQual', 'OverallCond',\n       'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd',\n       'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual',\n       'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n       'BsmtFinType2', 'Heating',\n       'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual',\n       'Functional', 'FireplaceQu', 'GarageType',\n       'GarageFinish', 'GarageQual',\n       'GarageCond', 'PavedDrive', 'PoolQC',\n       'Fence', 'MiscFeature', 'SaleType', 'SaleCondition']\nfor category_name in category_names:\n    all_data = pd.concat([initial_data[category_name], test_data[category_name]])\n    categories = pd.Series(all_data, dtype=\"category\")\n    initial_data[category_name] = initial_data[category_name].astype('category', categories).values\n    test_data[category_name] = test_data[category_name].astype('category', categories).values\n\n# One hot encode categorical columns\n# Need to join data so all categories are encoded the same way in both datasets\ninitial_data_length = len(initial_data.index)\ndata = pd.concat([initial_data, test_data], sort=True)\ndata = pd.get_dummies(data)\ninitial_data = data[:initial_data_length]\ntest_data = data[initial_data_length:]\n\n# Set up features\nstatic_column_names_to_drop = ['Id', 'SalePrice']\nencoded_categorical_column_names_to_drop = [x for x in data.columns.values if x.startswith((\"SaleType\", \"SaleCondition\"))]\npredictor_names = data.columns.drop(static_column_names_to_drop + encoded_categorical_column_names_to_drop).values\nX_initial = initial_data[predictor_names]\nX_test = test_data[predictor_names]\n\n# Set up target\ntarget_name = \"SalePrice\"\ny_initial = np.log(initial_data[target_name])\n\n# Split datasets\nX_train, X_validate, y_train, y_validate = train_test_split(X_initial, y_initial, random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Examine data/features\n\nprint(\"Features\")\nprint(predictor_names)\nprint(\"\\n\")\n\nprint(X_train.head())\nprint(X_train.describe())\nprint(\"\\n\")\n\nprint(y_train.head())\nprint(y_train.describe())\nprint(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"238961fc181052852a4060305ba7feeda36d3a8a"},"cell_type":"code","source":"# Build pipeline components\n\nimputer = SimpleImputer()\ncreateEstimator = (lambda params = {}: XGBRegressor(random_state=random_state, **params))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad76288faeae717d512171f02d30f2a0fa3cc340","_kg_hide-input":false,"scrolled":true},"cell_type":"code","source":"# # Find optimal parameters (only needs running once for each model change)\n\n# fixedParams = {\n#     \"n_estimators\": 550,\n#     \"max_depth\": 3,\n# #     \"early_stopping_rounds\": 1,\n# #     \"learning_rate\": 0.1,\n#     \"eval_set\": [(X_validate, y_validate)],\n# }\n\n# base_pipeline = Pipeline([(\"imputer\", imputer), (\"estimator\", createEstimator(fixedParams))])\n\n# param_grid = {\n# #     \"estimator__n_estimators\": [300, 500, 550, 600, 800],\n#     \"estimator__early_stopping_rounds\": [1, 2, 3],\n#     \"estimator__learning_rate\": [0.08, 0.1, 0.12],\n# #     \"estimator__max_depth\": [1, 3, 5, 10],\n# }\n# grid = GridSearchCV(base_pipeline, param_grid, n_jobs=-1, cv=5)\n# grid.fit(X_train, y_train)\n\n# pipeline = grid.best_estimator_\n\n# print(pipeline.get_params().keys())\n# print(grid.best_score_)\n# print(grid.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16e9159bfce529b1d6284291e9d2b2341d07b79d"},"cell_type":"code","source":"# Quick model training (only run once optimal parameters have been found)\n\nparams = {\n    \"n_estimators\": 550,\n    \"max_depth\": 3,\n    \"early_stopping_rounds\": 1,\n    \"learning_rate\": 0.1,\n}\npipeline = Pipeline([(\"imputer\", imputer), (\"estimator\", createEstimator(params))])\npipeline.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5187410f3a5e3013c190570cff0f7ba05cd4185e","scrolled":true},"cell_type":"code","source":"# Analyse\n\npredictions = pipeline.predict(X_validate)\n\nreadable_predictions = np.exp(predictions)\nreadable_y_validate = np.exp(y_validate)\n\nfrom sklearn.metrics import mean_squared_log_error\nrmsle = np.sqrt(mean_squared_log_error(readable_y_validate, readable_predictions))\nprint(\"Root Mean Squared Log Error\")\nprint(rmsle)\nprint(\"\")\n\nprint(\"First 5\")\nprint([float(x) for x in readable_predictions[:5]])\nprint([float(x) for x in readable_y_validate[:5]])\nprint(\"\")\n\nprint(\"Last 5\")\nprint([float(x) for x in readable_predictions[-5:]])\nprint([float(x) for x in readable_y_validate[-5:]])\nprint(\"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b06d960d0e590430bc65e0d2e2531753cb594e3e","scrolled":true},"cell_type":"code","source":"# Graph: Actual vs predicted prices\nwarnings.filterwarnings(\"ignore\")\n\nprices_joint_grid = sns.jointplot(x=readable_y_validate, y=readable_predictions, kind=\"reg\", height=20);\nprices_joint_grid.set_axis_labels('Actual prices ($)', 'Predicted prices ($)')\n\nwarnings.resetwarnings()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fddc1d2a8bbdd3b320d6b9873cde0085a9d6d36d"},"cell_type":"code","source":"# Graph: Feature importances\nfrom xgboost import plot_importance\n\n# Column names are lost in the pipeline somewhere, so re-add them to the plot\n# Assumption here is that column order is preserved even when the names are lost :fingers_crossed:\ncolumn_names = X_train.columns.values\nlabel_map = {\"f{0}\".format(index): name for index, name in enumerate(column_names)}\nfscore = pipeline.named_steps['estimator'].get_booster().get_fscore()\nnamed_values = {label_map[k]: v for k, v in fscore.items()}\n\nimportance_fig, importance_ax = plt.subplots()\nplot_importance(named_values, ax=importance_ax, color='red')\nimportance_fig.set_size_inches((20, 50))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af9589ed50b5f06e7e6083fe5d010f64bc44ebfc"},"cell_type":"markdown","source":"Merging train & validation data for final model seems to make the final model worse, not better! Why?\n```python\nX = pd.concat([X_train, X_validate])\ny = pd.concat([y_train, y_validate])\npipeline.fit(X, y)\n```"},{"metadata":{"trusted":true,"_uuid":"ddfbe470f3edc4559224ac698edb4b6a8a78de55"},"cell_type":"code","source":"# Submit\ntest_predictions = pipeline.predict(X_test)\nsubmission = pd.DataFrame({'Id': test_data.Id, 'SalePrice': np.exp(test_predictions)})\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a1a1b87b099f26ebf9c2d24d6ce14216c9075158"},"cell_type":"markdown","source":"## Notes\n\n* Don't need to manually split CV data with `StratifiedKFold` or similar, it's done automatically by `GridSearchCV` (defaults to 3-fold).\n\n## References\n\n* [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n* [RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)\n* [Mean squared log error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_log_error.html)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}